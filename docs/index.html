

<!DOCTYPE html>
<html>
<head>
	<title>Pre-training without Natural Images</title>
    <link rel="stylesheet" type="text/css" href="./pvg.css">
    <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
    <!--<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">-->
</head>

<body>
<script type="text/javascript" src="./header.js"></script>

<style>
a.myclass {
    color:#DE382D;
    text-decoration: underline
}
</style>

<style>
a.link {
    text-decoration: underline
}
</style>


<h1 align="center" style="font-size: 30pt;"><b>Pre-training without Natural Images</b></h1><br/>

<center>
    <font color="#c7254e"><b>International Journal of Computer Vision (IJCV)</b><br><b>ACCV 2020 Best Paper Honorable Mention Award (Oral, 3 strong accepts)</b></font><br><br>
    <!--<font color="#c7254e">Asian Conference on Computer Vision (ACCV) 2020<br><b>Best Paper Honorable Mention Award</b><br>Oral Presentation, The paper got 3 strong accepts</font><br><br>-->
    <a href="http://hirokatsukataoka.net/" class="">Hirokatsu Kataoka</a><sup>1</sup> &emsp; <a href="https://twitter.com/cv2aaa" class="">Kazushige Okayasu</a><sup>1,2</sup> &emsp; Asato Matsumoto<sup>1,3</sup> &emsp; Eisuke Yamagata<sup>4</sup><br>
    <a href="https://twitter.com/FragileGoodwill" class="">Ryosuke Yamada</a><sup>1,2</sup> &emsp; <a href="https://mmai.tech/" class="">Nakamasa Inoue</a><sup>4</sup> &emsp; <a href="http://www.is.fr.dendai.ac.jp/"  class="">Akio Nakamura</a><sup>2</sup> &emsp; <a href="https://staff.aist.go.jp/yu.satou/"  class="">Yutaka Satoh</a><sup>1,3</sup><br>
    1: AIST &emsp; 2: TDU &emsp; 3: Univ. of Tsukuba &emsp; 4: TITech<br><br>
    
    <section class="delta">
        <div class="container">
            <a href="https://link.springer.com/content/pdf/10.1007/s11263-021-01555-8.pdf"><button class="btn btn-gray">Paper (IJCV)</button></a>
            <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Kataoka_Pre-training_without_Natural_Images_ACCV_2020_paper.pdf"><button class="btn btn-gray">Paper (ACCV)</button></a>
            <a href="https://github.com/hirokatsukataoka16/FractalDB-Pretrained-ResNet-PyTorch"><button class="btn btn-gray">Code</button></a>
            <a href="#dataset"><button class="btn btn-gray">Dataset</button></a> <br>
            <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_oral.pdf"><button class="btn btn-gray">Oral</button></a>
            <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_poster.pdf"><button class="btn btn-gray">Poster</button></a>
            <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_fractaldb_supplementary.pdf"><button class="btn btn-gray">Supp</button></a>
            <a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/"><button class="btn btn-gray">Related Work</button></a>
        </div>
    </section>
    <!--<a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Kataoka_Pre-training_without_Natural_Images_ACCV_2020_paper.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Paper</a>
    <a href="https://github.com/hirokatsukataoka16/FractalDB" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Code</a>
    <a href="#dataset" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Dataset</a>
    <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_oral.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Oral</a>
    <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_poster.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Poster</a>
    <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_fractaldb_supplementary.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Supp. Mat.</a>
    <a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Related Work</a>-->
    <br><br>
    <iframe width="800" height="450" src="https://www.youtube.com/embed/d-NagM4nGIQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <!--<img src="./img/teaser.png" style="width: 100%;"/>-->
</center>

<br>
<h2>Abstract</h2>
<p>
Is it possible to use convolutional neural networks pre-trained without any natural images to assist natural image understanding? The paper proposes a novel concept, Formula-driven Supervised Learning (FDSL). We automatically generate image patterns and their category labels by assigning fractals, which are based on a natural law. Theoretically, the use of automatically generated images instead of natural images in the pre-training phase allows us to generate an infinitely large dataset of labeled images. The proposed framework is similar yet different from Self-Supervised Learning because the FDSL framework enables the creation of image patterns based on any mathematical formulas in addition to self-generated labels. Further, unlike pre-training with a synthetic image dataset, a dataset under the framework of FDSL is not required to define object categories, surface texture, lighting conditions, and camera viewpoint. In the experimental section, we find a better dataset configuration through an exploratory study, e.g., increase of #category/#instance, patch rendering, image coloring, and training epoch. Although models pre-trained with the proposed Fractal DataBase (FractalDB), a database without natural images, do not necessarily outperform models pre-trained with human annotated datasets in all settings, we are able to partially surpass the accuracy of ImageNet/Places pre-trained models. The FractalDB pre-trained CNN also outperforms other pre-trained models on auto-generated datasets based on FDSL such as Bezier curves and Perlin noise. This is reasonable since natural objects and scenes existing around us are constructed according to fractal geometry. Image representation with the proposed FractalDB captures a unique feature in the visualization of convolutional layers and attentions.
</p>

<br>
<h2>Framework</h2>
Proposed pre-training without natural images based on fractals, which is a natural formula existing in the real world (Formula-driven Supervised Learning). We automatically generate a large-scale labeled image dataset based on an iterated function system (IFS). (Bottom-left image)
The pre-training framework with Fractal geometry for feature representation learning. We can enhance natural image recognition by pre-training without natural images. (Bottom-right image) Accuracy transition among ImageNet-1k, FractalDB-1k and training from scratch.
<br><br><br>
<center>
        <img src="./img/framework.png" style="width: 100%;"/>
</center>

<br><br><br>
<h2>Experimental Results</h2>

We compared Scratch from random parameters, Places-30/365, ImageNet-100/1k (ILSVRCâ€™12), and FractalDB-1k/10k in the following table. Since our implementation is not completely the same as a representative learning configuration, we implemented the framework fairly with the same parameters and compared the proposed method (FractalDB-1k/10k) with a baseline (Scratch, DeepCluster-10k, Places-30/365, and ImageNet-100/1k). The  proposed  FractalDB  pre-trained  model  recorded  several  good  performance rates. We respectively describe them by comparing our Formula-driven Supervised Learning with Scratch, Self-supervised and Supervised Learning.
<br>
<center>
        <img src="./img/results.png" style="width: 85%;"/>
</center>


<br><br><br>
<h2>Visual Results</h2>

The figures show the activation of the 1st convolutional layer
on ResNet-50 at each pre-training model.
<br>
<center>
        <img src="./img/conv1.png" style="width: 85%;"/>
</center>

<br>
<h2>Citation</h2>
@article{KataokaIJCV2022,<br>
&emsp;author     = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},<br>
&emsp;title      = {Pre-training without Natural Images},<br>
&emsp;journal    = {International Journal of Computer Vision (IJCV)},<br>
&emsp;year       = {2022}<br>
}
<br><br>
@inproceedings{KataokaACCV2020,<br>
&emsp;author     = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},<br>
&emsp;title      = {Pre-training without Natural Images},<br>
&emsp;booktitle    = {Asian Conference on Computer Vision (ACCV)},<br>
&emsp;year       = {2020}<br>
}
<br><br>

<a name="dataset"><h2>Dataset Download</h2></a>
<ul>
    <li>
        FractalDB-1k (1k categories x 1k instances; Total 1M images).
        <a href="https://drive.google.com/file/d/1KKqz0H7i_TXFMa2oJtcfry9bmAxyS_SS/view?usp=sharing">[Dataset (13GB)]</a>
    </li>
    <li>
        FractalDB-60 (60 well-known categories x 1k instances; Total 60k images).
        <a href="https://drive.google.com/file/d/1F0aEogTScpABjJhNZJaCFT-J8mkdP86o/view?usp=sharing">[Dataset (1.2GB)]</a>
    </li>
</ul>
<br><br>

<h2>Acknowledgement</h2></a>
<ul>
    <li> This work is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO).</li>
    <li> This work was supported by JSPS KAKENHI Grant Number JP19H01134.</li>
    <li> Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.</li>
</ul>

<br><br><br>
<script type="text/javascript" src="./footer.js"></script>
</body></html>